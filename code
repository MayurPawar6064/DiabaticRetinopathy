import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import json
import os

# Load your dataset paths
trainPath = "/media/ratnapal/sda3/group_12_dataset/preprocessed dataset/training"
testPath = "/media/ratnapal/sda3/group_12_dataset/preprocessed dataset/testing"

# Data augmentation for the training set
train_datagen = ImageDataGenerator(
    rescale=1./255,              # Rescale pixel values from [0, 255] to [0, 1]
    shear_range=0.2,             # Randomly shear images
    zoom_range=0.2,              # Randomly zoom into images
    rotation_range=40,           # Rotate images randomly
    width_shift_range=0.2,       # Horizontally shift images
    height_shift_range=0.2,      # Vertically shift images
    horizontal_flip=True,        # Flip images horizontally
    validation_split=0.2         # Use 20% of data for validation
)

# No augmentation for the validation set, just rescaling
test_datagen = ImageDataGenerator(rescale=1./255)

# Prepare training and validation sets with batch size of 16
training_set = train_datagen.flow_from_directory(
    trainPath,
    target_size=(299, 299),      # Resize all images to 299x299
    batch_size=32,               # Batch size of 16
    class_mode='categorical',    # Multi-class classification
    subset="training"            # 80% training data
)

validation_set = train_datagen.flow_from_directory(
    trainPath,
    target_size=(299, 299),
    batch_size=32,               # Batch size of 16
    class_mode='categorical',    # Multi-class classification
    subset="validation"          # 20% validation data
)

# Step 1: Define the CNN model
model = Sequential()

# 1st Convolution Layer
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(299, 299, 3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

# 2nd Convolution Layer
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

# 3rd Convolution Layer
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

# 4th Convolution Layer
model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

model.add(Conv2D(512, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

# Flatten the output
model.add(Flatten())

# Fully connected layer
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.3))  # Dropout to reduce overfitting

# Output Layer (for 5 classes)
model.add(Dense(5, activation='softmax'))

# Step 2: Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Model architecture summary
model.summary()

# Train the model for 100 epochs
epochs = 100
history_data = {
    "accuracy": [],
    "val_accuracy": [],
    "loss": [],
    "val_loss": []
}

# Training the model for 100 epochs
print("Training the model for 100 epochs...")
history = model.fit(
    training_set,
    epochs=epochs,
    validation_data=validation_set,
    # Uncomment the next line to use early stopping if needed
    # callbacks=[early_stopping]
)

# Append the history data after the training is complete
history_data["accuracy"] = history.history['accuracy']
history_data["val_accuracy"] = history.history['val_accuracy']
history_data["loss"] = history.history['loss']
history_data["val_loss"] = history.history['val_loss']

# Save the training history data to JSON file
json_file_path = "training_history.json"
with open(json_file_path, "w") as json_file:
    json.dump(history_data, json_file)

# Plot training history (accuracy and loss)
plt.figure(figsize=(12, 4))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(history_data['accuracy'], label='Training Accuracy')
plt.plot(history_data['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy (100 Epochs)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history_data['loss'], label='Training Loss')
plt.plot(history_data['val_loss'], label='Validation Loss')
plt.title('Model Loss (100 Epochs)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')

# Show the plots
plt.show()

# Final evaluation of the model after 100 epochs
val_loss, val_acc = model.evaluate(validation_set)
print(f"Final Validation Loss: {val_loss}")
print(f"Final Validation Accuracy: {val_acc}")

print("Training history saved to 'training_history.json'.")
